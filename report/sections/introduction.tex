\section{Introduction}\label{sec:introduction}
Reinforcement Learning (RL) has emerged as a powerful paradigm for training agents to make sequential decisions in complex environments. Among the various RL algorithms, Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} stands out due to its robust performance, sample efficiency, and relative ease of implementation. PPO has become a popular choice for a wide range of control tasks, from simple game environments to complex robotic simulations.

This report details the implementation and evaluation of the PPO algorithm. The primary objective was to develop a functional PPO agent and assess its capabilities across a selection of benchmark environments from the Gymnasium library~\cite{gymnasium2023}. Specifically, we test our implementation on CartPole-v1, a classic discrete control problem, and on two more challenging continuous control tasks from the MuJoCo suite: HalfCheetah-v5 and Reacher-v5.

Our PPO implementation incorporates key algorithmic components such as an actor-critic architecture, Generalized Advantage Estimation (GAE)~\cite{schulman2015high} for stable advantage calculation, and the characteristic clipped surrogate objective function of PPO to ensure stable policy updates. The agent's neural networks utilize Tanh activation functions, observation normalization, and specific weight initialization techniques inspired by established practices~\cite{PPOcode}.

The implemented agent was successfully trained on the selected environments, achieving performance levels comparable to those reported in the original PPO paper~\cite{schulman2017proximal}. To complement the quantitative results, an interactive Streamlit application was developed, allowing for the visualization of the trained agents' policies in their respective environments.

This report is structured as follows: Section~\ref{sec:methodology} describes the PPO algorithm, the specifics of our implementation, and details of the environments used. Section~\ref{sec:results} presents the experimental results, including learning curves and performance comparisons. Finally, Section~\ref{sec:conclusion} summarizes the findings and discusses potential avenues for future work.